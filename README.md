https://www.youtube.com/watch?v=lp3u_eRbFuc
This is a video of how my project works in real time,


This project is a real-time American Sign Language (ASL) translator built using computer vision and classical machine learning. Using a webcam, the system detects and interprets ASL finger-spelling gestures and converts them into live text output to assist with non-verbal communication.

It uses MediaPipe for hand tracking, OpenCV for frame processing, and a Random Forest classifier trained on over 5,000 labeled hand landmark samples to recognize the 26 ASL letters with over 95% accuracy. The system includes smoothing logic, a gesture hold timer, and live sentence construction to ensure smooth and accurate translation for users.

This project focuses on accessibility, real-time interaction, and building a bridge between spoken and signed communication.

